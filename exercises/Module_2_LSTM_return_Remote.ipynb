{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Module_2_LSTM_return_Remote.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ET6ZsKtPRq6s","colab_type":"text"},"source":["## How to use return_state or return_sequences in Keras"]},{"cell_type":"markdown","metadata":{"id":"IaDAjPItRnV_","colab_type":"text"},"source":["Ref: https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/"]},{"cell_type":"markdown","metadata":{"id":"kRh2JLxIYC2x","colab_type":"text"},"source":["c<t> for each RNN cell in the above formulas is known as the cell state. For GRU, a given time step's cell state equals to its output hidden state. For LSTM, the output hidden state a<t> is produced by \"gating\" cell state c<t> by the output gate , so a<t> and c<t> are not the same. "]},{"cell_type":"markdown","metadata":{"id":"rmVZYOLyYDdE","colab_type":"text"},"source":["## Return sequences"]},{"cell_type":"markdown","metadata":{"id":"PVe5EOIaYVI7","colab_type":"text"},"source":["Return sequences refer to return the hidden state a<t>. By default, the return_sequences is set to False in Keras RNN layers, and this means the RNN layer will only return the last hidden state output a<T>. The last hidden state output captures an abstract representation of the input sequence. \n","  \n","In some case, it is all we need, such as a classification or regression model where the RNN is followed by the Dense layer(s) to generate logits for news topic classification or score for sentiment analysis, or in a generative model to produce the softmax probabilities for the next possible char.\n","  \n","In other cases, we need the full sequence as the output. Setting return_sequences to True is necessary."]},{"cell_type":"code","metadata":{"id":"yO9vcunoYhzb","colab_type":"code","colab":{}},"source":["from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import LSTM\n","from numpy import array\n","import keras\n","k_init = keras.initializers.Constant(value=0.1)\n","b_init = keras.initializers.Constant(value=0)\n","r_init = keras.initializers.Constant(value=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3AIhKXtZaE8","colab_type":"code","outputId":"d227e904-4368-4632-e838-8d21aa38e9b6","executionInfo":{"status":"ok","timestamp":1569498610539,"user_tz":-480,"elapsed":590,"user":{"displayName":"Anqi Tu","photoUrl":"","userId":"10241159283576056135"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# define input data\n","data = array([0.1, 0.2, 0.3, 0.1, 0.2, 0.3]).reshape((1,3,2))\n","data"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0.1, 0.2],\n","        [0.3, 0.1],\n","        [0.2, 0.3]]])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"x3egepCIYfGB","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import LSTM\n","\n","model = Sequential()\n","model.add(LSTM(1, return_sequences=True, kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbmSpgq_Z09P","colab_type":"code","outputId":"505669c4-811d-4602-a6aa-f90b43142248","executionInfo":{"status":"ok","timestamp":1569498612058,"user_tz":-480,"elapsed":860,"user":{"displayName":"Anqi Tu","photoUrl":"","userId":"10241159283576056135"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# make and show prediction\n","output = model.predict(data)\n","print(output, output.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[[0.00767819]\n","  [0.01597687]\n","  [0.02480671]]] (1, 3, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DvS0-rpfZq6Z","colab_type":"text"},"source":["We can see the output array's shape of the LSTM layer is (1,3,1) which stands for (#Samples, #Time steps, #LSTM units). Compared to when return_sequences is set to False, the shape will be (#Samples, #LSTM units), which only returns the last time step hidden state."]},{"cell_type":"code","metadata":{"id":"eZlvxe2QaPdc","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import LSTM\n","\n","model = Sequential()\n","model.add(LSTM(1, kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwN2m2GhaV3p","colab_type":"code","outputId":"96e47722-1f6a-447e-f9c4-effe2909a2ac","executionInfo":{"status":"ok","timestamp":1569498690178,"user_tz":-480,"elapsed":871,"user":{"displayName":"Anqi Tu","photoUrl":"","userId":"10241159283576056135"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# make and show prediction\n","output = model.predict(data)\n","print(output, output.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.02480671]] (1, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rryc7aR2aW7S","colab_type":"text"},"source":["There are two primary situations when you can apply the return_sequences to return the full sequence.\n","\n","1. Stacking RNN, the former RNN layer or layers should set return_sequences to True so that the following RNN layer or layers can have the full sequence as input.\n","2. We want to generate classification for each time step.\n","- Such as speech recognition or much simpler form - trigger word detection where we generate a value between 0~1 for each timestep representing whether the trigger word is present.\n","- OCR(Optical character recognition) sequence modeling with CTC."]},{"cell_type":"markdown","metadata":{"id":"_W0BLy78ahXJ","colab_type":"text"},"source":["## Return states"]},{"cell_type":"markdown","metadata":{"id":"ANHmfXjIa8bL","colab_type":"text"},"source":["Return sequences refer to return the cell state c<t>. For GRU, a<t>=c<t>, so you can get around without this parameter. But for LSTM, hidden state and cell state are not the same."]},{"cell_type":"code","metadata":{"id":"eG42EwNJbDr8","colab_type":"code","colab":{}},"source":["inputs1 = Input(shape=(3, 2))\n","lstm1, state_h, state_c = LSTM(units, return_state=True, kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init)(inputs1)\n","model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i99KKA7pcI-O","colab_type":"code","colab":{}},"source":["# model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgC1TKEvbXec","colab_type":"code","outputId":"a7cd1d38-0fc8-4c96-e233-ee85a4209584","executionInfo":{"status":"ok","timestamp":1569499459549,"user_tz":-480,"elapsed":623,"user":{"displayName":"Anqi Tu","photoUrl":"","userId":"10241159283576056135"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# make and show prediction\n","output = model.predict(data)\n","output"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[0.02480671]], dtype=float32),\n"," array([[0.02480671]], dtype=float32),\n"," array([[0.0486485]], dtype=float32)]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"BKdf4c9pbX5X","colab_type":"code","outputId":"a5fa2511-65bf-41e0-b752-c02dfeedf545","executionInfo":{"status":"ok","timestamp":1569499525204,"user_tz":-480,"elapsed":620,"user":{"displayName":"Anqi Tu","photoUrl":"","userId":"10241159283576056135"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["for a in output:\n","    print(a.shape) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1, 1)\n","(1, 1)\n","(1, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PzlW0y6kdi2m","colab_type":"text"},"source":["The output of the LSTM layer has three components, they are (a<T>, a<T>, c<T>), \"T\" stands for the last timestep, each one has the shape (#Samples, #LSTM units)."]},{"cell_type":"markdown","metadata":{"id":"VXJm1yVXdVZ5","colab_type":"text"},"source":["The major reason you want to set the return_state is an RNN may need to have its cell state initialized with previous time step while the weights are shared, such as in an encoder-decoder model. "]},{"cell_type":"code","metadata":{"id":"KdfPD0sudgqo","colab_type":"code","colab":{}},"source":["# define model\n","inputs1 = Input(shape=(3, 2))\n","lstm1, state_h, state_c = LSTM(units, return_sequences=True, return_state=True, kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init)(inputs1)\n","model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7S8B_U9ndy7g","colab_type":"code","outputId":"077f64fe-109e-46e1-d679-20fca4ed4aad","executionInfo":{"status":"ok","timestamp":1569499647977,"user_tz":-480,"elapsed":760,"user":{"displayName":"Anqi Tu","photoUrl":"","userId":"10241159283576056135"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["output = model.predict(data)\n","output"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[[0.00767819],\n","         [0.01597687],\n","         [0.02480671]]], dtype=float32),\n"," array([[0.02480671]], dtype=float32),\n"," array([[0.0486485]], dtype=float32)]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"XrlHxSareAyj","colab_type":"code","outputId":"d17a6f28-026a-4aa3-d4b0-ca252be11729","executionInfo":{"status":"ok","timestamp":1569499651349,"user_tz":-480,"elapsed":676,"user":{"displayName":"Anqi Tu","photoUrl":"","userId":"10241159283576056135"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["for a in output:\n","    print(a.shape) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1, 3, 1)\n","(1, 1)\n","(1, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"13o9a28Md0ih","colab_type":"text"},"source":["One thing worth mentioning is that if we replace LSTM with GRU the output will have only two components. (a<1...T>, c<T>) since in GRU a<T>=c<T>."]}]}